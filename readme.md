## ğŸ§  Desafio 1: Criar uma ExperiÃªncia Conversacional de Verdade
Para esse desafio tÃ©cnico decidi fazer algumas coisas diferentes. Como a ideia do desafio Ã© vocÃªs entenderem como eu penso, aprendo e resolvo problemas, eu quis documentar todo o meu processo criativo desde quando eu comecei a pensar o projeto. EntÃ£o esse comeÃ§o do readme serÃ¡ focado justamente nisso, documentei todas minhas ideias, dificuldades e como consegui resolver os problemas que foram surgindo no desenvolvimento do projeto.

### ğŸ’¡ Ideia Inicial

- Como meu foco Ã© mais backend, queria usar a IA da v0 como aliada na parte de prototipaÃ§Ã£o e frontend.
- A ideia era criar um orÃ¡culo/assistente virtual que soubesse "tudo" sobre a FURIA de CS:GO.
- Fugir daquele padrÃ£o batido de CRUD.
- Dar personalidade pro assistente, transformando ele no "Furico", o mascote da FURIA.

---

### âš ï¸ Problema 1: InformaÃ§Ãµes atuais ou em tempo real

- Primeira tentativa: usar alguma IA pra acessar a internet direto e buscar os dados. Spoiler: nenhuma que encontrei oferecia isso via API.
- HLTV? Nada de API pÃºblica.
- Liquipedia? AtÃ© tem algo, mas sem documentaÃ§Ã£o clara e com termos de uso chatinhos.
- Segunda alternativa: montar meu prÃ³prio *web scraper* e alimentar uma IA com os dados extraÃ­dos.

Decidi seguir com o scraper, focado na pÃ¡gina da FURIA na HLTV. Sabia que isso traria alguns desafios:

- Escopo limitado de perguntas que poderiam ser respondidas.
- Respostas imprecisas.
- ComunicaÃ§Ã£o meio complicada entre os dados brutos e a IA.

Mesmo assim, parecia o caminho mais viÃ¡vel. JÃ¡ tinha experiÃªncia com a API da OpenAI, entÃ£o o grande desafio seria mesmo fazer um scraper robusto com dados bons e atualizados.

---

### ğŸ”§ CriaÃ§Ã£o do Scraper

- A ideia: montar um scraper que pegasse dados da FURIA e dos players na HLTV, salvando tudo em cache no Redis pra ficar rÃ¡pido e poder rodar o scraper separado da API principal.
- Bati de frente com a proteÃ§Ã£o da Cloudflare da HLTV. AÃ­ veio o plano B: Liquipedia.
- Liquipedia tinha dados bem diretos e menos proteÃ§Ã£o. Consegui fazer um scraper que pegasse:
  - Todos os resultados de partidas da FURIA.
  - A timeline de entrada e saÃ­da dos membros da equipe.

Assumi que isso jÃ¡ era suficiente pra um MVP.
**Melhorias futuras**: trazer estatÃ­sticas das partidas, dados dos jogadores e atÃ© infos da organizaÃ§Ã£o como um todo.

---

### ğŸ§  Como usar essas informaÃ§Ãµes de forma inteligente?

Beleza, dados coletados e salvos. Agora vinha a parte de como usar isso de forma otimizada para cobrir ao mÃ¡ximo a escolha de utilizar um scraper:

- A ideia era nÃ£o sobrecarregar a API da OpenAI com dados desnecessÃ¡rios.
- EntÃ£o pensei: cada mensagem do usuÃ¡rio deveria ser **classificada**, pra saber que tipo de informaÃ§Ã£o ela exige e de qual perÃ­odo ela deve ser buscada.

#### Categorias que defini:
- `matches`: perguntas sobre resultados de partidas.
- `timeline`: perguntas sobre lineup ou mudanÃ§as na equipe.
- `unknown`: perguntas sem dados suficientes pra responder.
- `default`: mensagens que nÃ£o precisam consultar a base de dados.

Com isso, a IA pode simplesmente classificar a pergunta do usuÃ¡rio e estimar um intervalo temporal pra busca.

##### Exemplo:
- Pergunta: â€œQual era a line da FURIA em 2020?â€
- A IA responde com: timeline;2020-2020
- A partir disso, faÃ§o a busca para encontrar os dados relevantes, e passo a pergunta somada com os dados refinados para o Furico.

Essa abordagem ajuda a economizar chamadas e tokens desnecessÃ¡rias Ã  IA e torna a conversa bem mais fluida e precisa. Caso eu nÃ£o utiliza-se um sistema de classificaÃ§Ã£o das perguntas eu teria que passar sempre todos os dados que foram coletados para a Cohere, o que nÃ£o iria funcionar pelas proprÃ­as restriÃ§Ãµes de caractÃ©res da plataforma.

> *Pequeno plot twist*: Acabou meu plano grÃ¡tis da OpenAI ğŸ˜“  
> Encontrei a Cohere e comecei a testar. TÃ¡ dando conta do recado!

Consegui montar essa estrutura do jeitinho que imaginei. Funcionando liso!

---

### ğŸ§‘â€ğŸ’» Desafio 2: SessÃ£o do UsuÃ¡rio (sem login chato)

- NÃ£o queria forÃ§ar o usuÃ¡rio a criar conta ou logar para utilizar a plataforma.
- Resolvi usar um UUID gerado e salvo em cookie como identificador da sessÃ£o.
- Essa sessÃ£o dura 24 horas e guarda o histÃ³rico de conversa.
- Implementei:
  - Armazenamento no Redis.
  - Middleware que gerencia a criaÃ§Ã£o/verificaÃ§Ã£o do cookie e UUID.
- Resultado: cada usuÃ¡rio tem uma conversa isolada e contÃ­nua (por 24h), sem precisar digitar um Ãºnico dado previamente.

---

### âœ… ConclusÃ£o

No fim das contas, esse projeto acabou sendo mais do que sÃ³ um desafio tÃ©cnico sobre a FURIA â€” virou um laboratÃ³rio pra testar ideias diferentes de como construir uma experiÃªncia de conversa mais natural e inteligente, fugindo do clÃ¡ssico CRUD. Apliquei vÃ¡rias ideias diferentes de como utilizar algumas ferramentas e tÃ©cnologias que nunca tinha pensado antes.

Consegui validar que dÃ¡ pra usar scraping, cache, classificaÃ§Ã£o de mensagens e IA de forma integrada, sem pesar pro usuÃ¡rio e mantendo uma performance legal. Ainda tem bastante coisa pra melhorar, mas como MVP, o Furico jÃ¡ mostra que tem potencial.

Por falta de tempo, nÃ£o consegui cobrir as possÃ­veis fragilidades do projeto, nÃ£o consegui implementar features que tinha planejado inicialmente como testes e documentaÃ§Ã£o da API com swagger, mas acredito que alcancei um resultado muito satisfatÃ³rio e essas melhorias ficarÃ£o para o futuro.

PrÃ³ximos passos? Adicionar mais dados, refinar as interaÃ§Ãµes, explorar outras IAs, testes, documentaÃ§Ã£o com swagger e desacoplar melhor algumas integraÃ§Ãµes no cÃ³digo para que o projeto sejÃ¡ mais facilmente escalÃ¡vel.

--- 

## ğŸ›  DocumentaÃ§Ã£o do Backend

### ğŸ“¦ Tecnologias utilizadas

- Node.js + Express  
- TypeScript  
- Redis  
- Puppeteer (para scraping)  
- Cohere (API de linguagem)  
- Redis para cache e armazenamento temporÃ¡rio  
- Docker (para subir o Redis)  
- ts-node + concurrently (para rodar API e tarefas agendadas em paralelo)

---

### ğŸš€ Como rodar o projeto do zero

#### 1. Entre na pasta do backend
```bash
cd backend
```

#### 2. Instale as dependÃªncias
```bash
npm install
```

#### 3. Crie um arquivo `.env` baseado no `.env.example`
```bash
cp .env.example .env
```

Edite o `.env` com a sua chave da API da Cohere:
```bash
API_KEY=suachaveaqui
PORT=8080
```

#### 4. Suba o Redis com Docker
VocÃª precisa ter o Docker instalado. Depois, Ã© sÃ³ rodar:
```bash
docker-compose up -d
```

#### 5. Inicie a aplicaÃ§Ã£o em modo desenvolvimento
O projeto jÃ¡ estÃ¡ preparado pra rodar a API e o cron (que atualiza os dados com scraping) ao mesmo tempo:
```bash
npm run dev
```
Esse comando executa dois serviÃ§os simultÃ¢neos:

- `start:server`: inicia o servidor Express

- `start:cron`: roda o job de scraping periÃ³dico

---

### ğŸ“‚ Estrutura de rotas
**POST** `/message`
Recebe uma mensagem do usuÃ¡rio e responde com base no histÃ³rico e dados salvos.

**GET** `/message`
Retorna a conversa atual do usuÃ¡rio, com base no UUID da sessÃ£o salva via cookie.

---

ğŸ“„ Scripts principais (`package.json`)
- `npm run start:server` â€“ Inicia apenas o servidor Express
- `npm run start:cron` â€“ Roda apenas os scrapers
- `npm run dev` â€“ Roda os dois ao mesmo tempo com o concurrently

---

### ğŸ’¾ Redis
Utilizado para:
- Cache de dados do scraper
- Armazenamento temporÃ¡rio de sessÃµes de usuÃ¡rio (identificadas via UUID em cookies)

---

## ğŸ¨ DocumentaÃ§Ã£o do Frontend

### ğŸ“¦ Tecnologias utilizadas

- Next.js 15  
- React 19  
- TypeScript  
- TailwindCSS  
- Radix UI (design system completo)  
- React Hook Form + Zod (validaÃ§Ãµes)  
- Recharts (grÃ¡ficos)  
- Embla Carousel  

---

### ğŸš€ Como rodar o projeto do zero

#### 1. Entre na pasta do frontend
```bash
cd frontend
```

#### 2. Instale as dependÃªncias
```bash
npm install
```

#### 3. Crie um arquivo `.env` baseado no `.env.example`
```bash
cp .env.example .env
```

Edite o valor da variÃ¡vel `NEXT_PUBLIC_API_URL` com a URL da sua API (backend):
```bash
NEXT_PUBLIC_API_URL=http://localhost:8080
```

#### 4. Rode o projeto localmente
```bash
npm run dev
```

---

### ğŸ“„ Scripts principais (`package.json`)
- `npm run dev` â€“ Inicia o projeto em modo desenvolvimento
- `npm run build` â€“ Gera a versÃ£o de produÃ§Ã£o
- `npm start` â€“ Inicia o app jÃ¡ buildado
- `npm run lint` â€“ Roda o linter

---

### ğŸ§µ IntegraÃ§Ãµes importantes
- Consome os endpoints do backend via NEXT_PUBLIC_API_URL
- Usa cookies para identificar sessÃµes de usuÃ¡rios
- Interface personalizÃ¡vel e responsiva com Radix UI + Tailwind